{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"PS5\"\n",
        "author: \"Shreya Shravini\"\n",
        "date: \"11/10/2024\"\n",
        "format: \n",
        "  pdf:\n",
        "    include-in-header: \n",
        "       text: |\n",
        "         \\usepackage{fvextra}\n",
        "         \\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n",
        "include-before-body:\n",
        "  text: |\n",
        "    \\RecustomVerbatimEnvironment{verbatim}{Verbatim}{\n",
        "      showspaces = false,\n",
        "      showtabs = false,\n",
        "      breaksymbolleft={},\n",
        "      breaklines\n",
        "    }\n",
        "output:\n",
        "  echo: false\n",
        "  eval: false\n",
        "---\n",
        "\n",
        "\n",
        "github link: https://github.com/shreyashravini/PS5\n",
        "\n",
        "\\newpage\n"
      ],
      "id": "8c72b00f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import altair as alt\n",
        "import time\n",
        "\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "alt.renderers.enable(\"png\")"
      ],
      "id": "825cba66",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Develop initial scraper and crawler\n",
        "\n",
        "### 1. Scraping (PARTNER 1)\n"
      ],
      "id": "a3ac9193"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# Define the URL of the HHS OIG Enforcement Actions page\n",
        "url = \"https://oig.hhs.gov/fraud/enforcement/\"\n",
        "\n",
        "# Send a request to retrieve the HTML content\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "# Filter out list items that match the specified class for enforcement actions\n",
        "enforcement_items = soup.find_all(\"li\", class_=\"usa-card card--list pep-card--minimal mobile:grid-col-12\")\n",
        "\n",
        "# Extract titles of enforcement actions\n",
        "action_titles = [item.find(\"h2\", class_=\"usa-card__heading\").find(\"a\").get_text(strip=True) for item in enforcement_items]\n",
        "\n",
        "# Extract dates associated with each action\n",
        "action_dates = [item.find(class_=\"text-base-dark padding-right-105\").get_text(strip=True) for item in enforcement_items]\n",
        "\n",
        "# Extract categories for each enforcement action\n",
        "action_categories = [item.find(\"ul\", class_=\"display-inline add-list-reset\").find(\"li\").get_text(strip=True) for item in enforcement_items]\n",
        "\n",
        "# Extract links for each enforcement action\n",
        "action_links = [item.find(\"a\")['href'] for item in enforcement_items if item.find(\"a\")]\n",
        "\n",
        "# Organize extracted data into a DataFrame\n",
        "data = {\n",
        "    \"Title\": action_titles,\n",
        "    \"Date\": action_dates,\n",
        "    \"Category\": action_categories,\n",
        "    \"Link\": action_links\n",
        "}\n",
        "\n",
        "enforcement_df = pd.DataFrame(data)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "enforcement_df.head()\n"
      ],
      "id": "99bdec5b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Crawling (PARTNER 1)\n"
      ],
      "id": "a4d056e9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from urllib.parse import urljoin\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "# Define the base URL for the HHS OIG Enforcement Actions page\n",
        "base_url = \"https://oig.hhs.gov/fraud/enforcement/\"\n",
        "\n",
        "# Convert relative links to absolute URLs\n",
        "full_urls = [urljoin(base_url, link) for link in action_links]\n",
        "print(full_urls)\n",
        "\n",
        "# Initialize a list to store agency names\n",
        "agency_names = []\n",
        "\n",
        "# Loop over each URL to access the page and retrieve the agency name\n",
        "for url in full_urls:\n",
        "    response = requests.get(url)\n",
        "    page_soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    \n",
        "    # Locate the specific <ul> tag containing the agency information\n",
        "    agency_section = page_soup.find_all(\"ul\", class_=\"usa-list usa-list--unstyled margin-y-2\")\n",
        "    \n",
        "    for ul in agency_section:\n",
        "        li_tags = ul.find_all(\"li\")\n",
        "        \n",
        "        # Ensure at least two list items are available, then extract the second item\n",
        "        if len(li_tags) > 1:\n",
        "            agency_text = li_tags[1].get_text(strip=True).replace(\"Agency:\", \"\").strip()\n",
        "            agency_names.append(agency_text)\n",
        "\n",
        "print(agency_names)\n",
        "\n",
        "# Create a dictionary to add the agency information to the DataFrame\n",
        "agency_data = {\"Agency\": agency_names}\n",
        "\n",
        "# Convert the dictionary to a DataFrame and concatenate with the original DataFrame\n",
        "updated_df = pd.concat([enforcement_df, pd.DataFrame(agency_data)], axis=1)\n",
        "\n",
        "# Display the first few rows of the updated DataFrame\n",
        "updated_df.head()"
      ],
      "id": "31b8d4da",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Making the scraper dynamic\n",
        "\n",
        "### 1. Turning the scraper into a function \n",
        "\n",
        "* a. Pseudo-Code (PARTNER 2)\n",
        "\n",
        "Define fetch_page(session, url)\n",
        "We will fetch and return the HTML content from the url.\n",
        "\n",
        "Define scrape_action_page(session, link)\n",
        "We will fetch the enforcement action page and extract the agency name (return \"N/A\" if not found).\n",
        "\n",
        "Define scrape_enforcement_actions(year, month)\n",
        "We will check if the year is before 2013 and stop if true.\n",
        "We will initialize variables, set the target date, and create an asynchronous session.\n",
        "We will loop through pages, fetch HTML, extract enforcement data, and queue tasks to scrape agency names.\n",
        "We will store data for actions from the target date and continue until no entries are found.\n",
        "We will return the collected data as a DataFrame.\n",
        "\n",
        "Define main()\n",
        "We will prompt for the year and month, call scrape_enforcement_actions, display the DataFrame, and save it as a CSV.\n",
        "\n",
        "Run the script\n",
        "We will execute the main() function with asyncio.run().\n",
        "\n",
        "* b. Create Dynamic Scraper (PARTNER 2)\n"
      ],
      "id": "6f219252"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import asyncio\n",
        "import aiohttp\n",
        "import nest_asyncio\n",
        "from datetime import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "async def fetch_page(session, url):\n",
        "    async with session.get(url) as response:\n",
        "        return await response.text()\n",
        "\n",
        "async def extract_agency_info(session, action_url):\n",
        "    html = await fetch_page(session, action_url)\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    agency_list = soup.find(\"ul\", class_=\"usa-list usa-list--unstyled margin-y-2\")\n",
        "    \n",
        "    if agency_list and len(agency_list.find_all(\"li\")) > 1:\n",
        "        agency = agency_list.find_all(\"li\")[1].get_text(strip=True).replace(\"Agency:\", \"\").strip()\n",
        "    else:\n",
        "        agency = \"N/A\"\n",
        "    return agency\n",
        "\n",
        "async def scrape_enforcement_data(year, month):\n",
        "    if year < 2013:\n",
        "        print(\"Please input a year >= 2013. Enforcement actions are listed only from 2013 onwards.\")\n",
        "        return None\n",
        "\n",
        "    base_url = \"https://oig.hhs.gov/fraud/enforcement/\"\n",
        "    page_number = 1\n",
        "    \n",
        "    titles, dates, categories, links, agencies = [], [], [], [], []\n",
        "    start_date = datetime(year, month, 1).date()\n",
        "    \n",
        "    connector = aiohttp.TCPConnector(ssl=False)\n",
        "    async with aiohttp.ClientSession(connector=connector) as session:\n",
        "        while True:\n",
        "            current_page_url = f\"{base_url}?page={page_number}\" if page_number > 1 else base_url\n",
        "            print(f\"Scraping: {current_page_url}\")\n",
        "            \n",
        "            html = await fetch_page(session, current_page_url)\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "            \n",
        "            enforcement_entries = soup.find_all(\"li\", class_=\"usa-card card--list pep-card--minimal mobile:grid-col-12\")\n",
        "            \n",
        "            if not enforcement_entries:\n",
        "                print(f\"No more entries found on page {page_number}. Exiting.\")\n",
        "                break\n",
        "            \n",
        "            action_tasks = []\n",
        "            for entry in enforcement_entries:\n",
        "                date_str = entry.find(class_=\"text-base-dark padding-right-105\").get_text(strip=True)\n",
        "                entry_date = datetime.strptime(date_str, \"%B %d, %Y\").date()\n",
        "                \n",
        "                if entry_date >= start_date:\n",
        "                    title = entry.find(\"h2\", class_=\"usa-card__heading\").find(\"a\").get_text(strip=True)\n",
        "                    category = entry.find(\"ul\", class_=\"display-inline add-list-reset\").find(\"li\").get_text(strip=True)\n",
        "                    link = urljoin(base_url, entry.find(\"a\")[\"href\"])\n",
        "                    \n",
        "                    titles.append(title)\n",
        "                    dates.append(date_str)\n",
        "                    categories.append(category)\n",
        "                    links.append(link)\n",
        "                    \n",
        "                    action_tasks.append(extract_agency_info(session, link))\n",
        "                else:\n",
        "                    agencies.extend(await asyncio.gather(*action_tasks))\n",
        "                    return pd.DataFrame({\n",
        "                        \"Title of Enforcement Action\": titles,\n",
        "                        \"Date\": dates,\n",
        "                        \"Category\": categories,\n",
        "                        \"Link\": links,\n",
        "                        \"Agency\": agencies\n",
        "                    })\n",
        "            \n",
        "            agencies.extend(await asyncio.gather(*action_tasks))\n",
        "            page_number += 1\n",
        "            await asyncio.sleep(1)\n",
        "    \n",
        "    return pd.DataFrame({\n",
        "        \"Title of Enforcement Action\": titles,\n",
        "        \"Date\": dates,\n",
        "        \"Category\": categories,\n",
        "        \"Link\": links,\n",
        "        \"Agency\": agencies\n",
        "    })\n",
        "\n",
        "async def main():\n",
        "    year, month = 2023, 1 \n",
        "    df_jan_2023 = await scrape_enforcement_data(year, month)\n",
        "    print(df_jan_2023)\n",
        "    \n",
        "    # Save to CSV\n",
        "    filename = f\"enforcement_actions_{year}_{month:02d}.csv\"\n",
        "    df_jan_2023.to_csv(filename, index=False)\n",
        "    print(f\"Data saved to {filename}\")\n",
        "    \n",
        "    return df_jan_2023\n",
        "\n",
        "# Running the script\n",
        "nest_asyncio.apply()\n",
        "if __name__ == \"__main__\":\n",
        "    df_jan_2023 = asyncio.run(main())"
      ],
      "id": "040b73f8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_jan_2023.head()"
      ],
      "id": "56066474",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"The total number of enforcement actions collected is {df_jan_2023.shape[0]}.\")\n",
        "\n",
        "# Details of the first scraped enforcement action (earliest)\n",
        "first_entry = df_jan_2023.iloc[0].to_dict()\n",
        "print(first_entry)"
      ],
      "id": "40034ed5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* c. Test Partner's Code (PARTNER 1)\n"
      ],
      "id": "be54f673"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import asyncio\n",
        "import aiohttp\n",
        "import nest_asyncio\n",
        "from datetime import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "async def fetch_page_content(session, url):\n",
        "    async with session.get(url) as response:\n",
        "        return await response.text()\n",
        "\n",
        "async def extract_agency_info(session, action_url):\n",
        "    html = await fetch_page_content(session, action_url)\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    agency_list = soup.find(\"ul\", class_=\"usa-list usa-list--unstyled margin-y-2\")\n",
        "    \n",
        "    if agency_list and len(agency_list.find_all(\"li\")) > 1:\n",
        "        agency = agency_list.find_all(\"li\")[1].get_text(strip=True).replace(\"Agency:\", \"\").strip()\n",
        "    else:\n",
        "        agency = \"N/A\"\n",
        "    return agency\n",
        "\n",
        "async def scrape_enforcement_data_from(year, month):\n",
        "    if year < 2013:\n",
        "        print(\"Please choose a year >= 2013. Only enforcement actions after 2013 are available.\")\n",
        "        return None\n",
        "\n",
        "    base_url = \"https://oig.hhs.gov/fraud/enforcement/\"\n",
        "    page_number = 1\n",
        "    \n",
        "    titles, dates, categories, links, agencies = [], [], [], [], []\n",
        "    start_date = datetime(year, month, 1).date()\n",
        "    \n",
        "    connector = aiohttp.TCPConnector(ssl=False)\n",
        "    async with aiohttp.ClientSession(connector=connector) as session:\n",
        "        while True:\n",
        "            current_page_url = f\"{base_url}?page={page_number}\" if page_number > 1 else base_url\n",
        "            print(f\"Scraping page: {current_page_url}\")\n",
        "            \n",
        "            html = await fetch_page_content(session, current_page_url)\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "            \n",
        "            enforcement_entries = soup.find_all(\"li\", class_=\"usa-card card--list pep-card--minimal mobile:grid-col-12\")\n",
        "            \n",
        "            if not enforcement_entries:\n",
        "                print(f\"No more data found on page {page_number}. Stopping.\")\n",
        "                break\n",
        "            \n",
        "            action_tasks = []\n",
        "            for entry in enforcement_entries:\n",
        "                date_str = entry.find(class_=\"text-base-dark padding-right-105\").get_text(strip=True)\n",
        "                entry_date = datetime.strptime(date_str, \"%B %d, %Y\").date()\n",
        "                \n",
        "                if entry_date >= start_date:\n",
        "                    title = entry.find(\"h2\", class_=\"usa-card__heading\").find(\"a\").get_text(strip=True)\n",
        "                    category = entry.find(\"ul\", class_=\"display-inline add-list-reset\").find(\"li\").get_text(strip=True)\n",
        "                    link = urljoin(base_url, entry.find(\"a\")[\"href\"])\n",
        "                    \n",
        "                    titles.append(title)\n",
        "                    dates.append(date_str)\n",
        "                    categories.append(category)\n",
        "                    links.append(link)\n",
        "                    \n",
        "                    action_tasks.append(extract_agency_info(session, link))\n",
        "                else:\n",
        "                    agencies.extend(await asyncio.gather(*action_tasks))\n",
        "                    return pd.DataFrame({\n",
        "                        \"Title of Enforcement Action\": titles,\n",
        "                        \"Date\": dates,\n",
        "                        \"Category\": categories,\n",
        "                        \"Link\": links,\n",
        "                        \"Agency\": agencies\n",
        "                    })\n",
        "            \n",
        "            agencies.extend(await asyncio.gather(*action_tasks))\n",
        "            page_number += 1\n",
        "            await asyncio.sleep(1)\n",
        "    \n",
        "    return pd.DataFrame({\n",
        "        \"Title of Enforcement Action\": titles,\n",
        "        \"Date\": dates,\n",
        "        \"Category\": categories,\n",
        "        \"Link\": links,\n",
        "        \"Agency\": agencies\n",
        "    })\n",
        "\n",
        "async def main():\n",
        "    year, month = 2021, 1 \n",
        "    enforcement_df_Jan_2021 = await scrape_enforcement_data_from(year, month)\n",
        "    print(enforcement_df_Jan_2021)\n",
        "    \n",
        "    # Save the DataFrame to a CSV file\n",
        "    filename = f\"enforcement_actions_{year}_{month:02d}.csv\"\n",
        "    enforcement_df_Jan_2021.to_csv(filename, index=False)\n",
        "    print(f\"Data saved to {filename}\")\n",
        "    \n",
        "    return enforcement_df_Jan_2021\n",
        "\n",
        "# Running the script\n",
        "nest_asyncio.apply()\n",
        "if __name__ == \"__main__\":\n",
        "    enforcement_df_Jan_2021 = asyncio.run(main())"
      ],
      "id": "19d180fa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "enforcement_df_Jan_2021.head()"
      ],
      "id": "ad351885",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"The total number of enforcement actions in the dataframe is {enforcement_df_Jan_2021.shape[0]}.\")\n",
        "\n",
        "# Getting details of the earliest scraped enforcement action\n",
        "first_entry = enforcement_df_Jan_2021.iloc[0].to_dict()\n",
        "print(first_entry)"
      ],
      "id": "fb3c9252",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Plot data based on scraped data\n",
        "\n",
        "### 1. Plot the number of enforcement actions over time (PARTNER 2)\n"
      ],
      "id": "a8f8b15f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import altair as alt\n",
        "\n",
        "# Convert 'Date' column to datetime format\n",
        "enforcement_df_Jan_2021['Date'] = pd.to_datetime(enforcement_df_Jan_2021['Date'], errors='coerce')\n",
        "\n",
        "# Aggregate the data by month and year, then count the actions\n",
        "monthly_enforcement_counts = (\n",
        "    enforcement_df_Jan_2021\n",
        "    .groupby(enforcement_df_Jan_2021['Date'].dt.to_period('M'))\n",
        "    .size()\n",
        "    .reset_index(name='Action Count')\n",
        ")\n",
        "\n",
        "# Convert 'Date' to the first day of the respective month\n",
        "monthly_enforcement_counts['Date'] = monthly_enforcement_counts['Date'].dt.to_timestamp()\n",
        "\n",
        "# Generate the line chart using Altair\n",
        "action_trend_chart = alt.Chart(monthly_enforcement_counts).mark_line(point=True).encode(\n",
        "    x=alt.X('Date:T', title='Month-Year', axis=alt.Axis(format='%Y-%m', labelAngle=-45)),\n",
        "    y=alt.Y('Action Count:Q', title='Total Actions'),\n",
        "    tooltip=['Date:T', 'Action Count:Q']\n",
        ").properties(\n",
        "    title='Enforcement Actions Trend by Month (From January 2021)',\n",
        "    width=750,\n",
        "    height=400\n",
        ")\n",
        "\n",
        "action_trend_chart.display()"
      ],
      "id": "8d54b466",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Plot the number of enforcement actions categorized: (PARTNER 1)\n",
        "\n",
        "* based on \"Criminal and Civil Actions\" vs. \"State Enforcement Agencies\"\n"
      ],
      "id": "8658c050"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Extract Year from Date\n",
        "enforcement_df_Jan_2021['Year'] = enforcement_df_Jan_2021['Date'].dt.year\n",
        "\n",
        "# Filter data to include only relevant categories\n",
        "enforcement_subset = enforcement_df_Jan_2021[\n",
        "    (enforcement_df_Jan_2021['Category'] == \"Criminal and Civil Actions\") |\n",
        "    (enforcement_df_Jan_2021['Category'] == \"State Enforcement Agencies\")\n",
        "]\n",
        "\n",
        "# Group the data by Month-Year and Category, and count occurrences\n",
        "category_count = enforcement_subset.groupby([enforcement_subset['Date'].dt.to_period(\"M\"), 'Category']).size().reset_index(name='Action Count')\n",
        "\n",
        "# Standardize 'Date' column to first day of the month\n",
        "category_count['Date'] = category_count['Date'].dt.to_timestamp()\n",
        "\n",
        "# Create line chart to visualize enforcement actions by category\n",
        "category_line_chart = alt.Chart(category_count).mark_line(point=True).encode(\n",
        "    x=alt.X('Date:T', title='Month-Year', axis=alt.Axis(format='%Y-%m', labelAngle=-45)),\n",
        "    y=alt.Y('Action Count:Q', title='Total Actions'),\n",
        "    color=alt.Color('Category:N', title='Action Type'),\n",
        "    tooltip=['Date:T', 'Category:N', 'Action Count:Q']\n",
        ").properties(\n",
        "    title='Enforcement Actions by Category Over Time',\n",
        "    width=700,\n",
        "    height=400\n",
        ")\n",
        "\n",
        "category_line_chart.display()\n"
      ],
      "id": "5ab43d57",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* based on five topics\n"
      ],
      "id": "3aadfb36"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define a function to categorize actions based on title keywords\n",
        "def categorize_action_by_topic(title):\n",
        "    title = title.lower()\n",
        "    if \"health\" in title:\n",
        "        return \"Health Care Fraud\"\n",
        "    elif \"financial\" in title or \"bank\" in title:\n",
        "        return \"Financial Fraud\"\n",
        "    elif \"drug\" in title:\n",
        "        return \"Drug Enforcement\"\n",
        "    elif \"bribery\" in title or \"corruption\" in title:\n",
        "        return \"Bribery/Corruption\"\n",
        "    else:\n",
        "        return \"Other\"\n",
        "\n",
        "# Apply the function to assign topics to relevant actions\n",
        "enforcement_df_Jan_2021['Topic'] = enforcement_df_Jan_2021.apply(\n",
        "    lambda row: categorize_action_by_topic(row['Title of Enforcement Action']) if row['Category'] == \"Criminal and Civil Actions\" else None, axis=1)\n",
        "\n",
        "# Filter data to include only \"Criminal and Civil Actions\" category\n",
        "criminal_civil_subset = enforcement_df_Jan_2021[enforcement_df_Jan_2021['Category'] == 'Criminal and Civil Actions']\n",
        "\n",
        "# Group data by month-year and topic, counting the occurrences\n",
        "topic_count = criminal_civil_subset.groupby([criminal_civil_subset['Date'].dt.to_period(\"M\"), 'Topic']).size().reset_index(name='Action Count')\n",
        "\n",
        "# Standardize 'Date' column to first day of the month\n",
        "topic_count['Date'] = topic_count['Date'].dt.to_timestamp()\n",
        "\n",
        "# Create line chart for topics within 'Criminal and Civil Actions'\n",
        "topic_line_chart = alt.Chart(topic_count).mark_line(point=True).encode(\n",
        "    x='Date:T',\n",
        "    y='Action Count:Q',\n",
        "    color='Topic:N',\n",
        "    tooltip=['Date:T', 'Topic:N', 'Action Count:Q']\n",
        ").properties(\n",
        "    title=\"Enforcement Actions by Topic (Criminal and Civil Actions)\",\n",
        "    width=700,\n",
        "    height=400\n",
        ").interactive()\n",
        "\n",
        "topic_line_chart.display()"
      ],
      "id": "afcb3c50",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Create maps of enforcement activity\n",
        "\n",
        "### 1. Map by State (PARTNER 1)\n"
      ],
      "id": "21b082d8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Importing the required libraries\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Filter out the enforcement actions taken by state-level agencies\n",
        "state_agency_data = enforcement_df_Jan_2021[enforcement_df_Jan_2021['Agency'].str.contains(\"State of\", na=False)]\n",
        "\n",
        "# Clean up the state names by removing \"State of\" prefix\n",
        "state_agency_data['State'] = state_agency_data['Agency'].str.replace(\"State of \", \"\").str.strip()\n",
        "\n",
        "# Group the data by state and count the number of enforcement actions per state\n",
        "state_action_counts = state_agency_data.groupby('State').size().reset_index(name='action_count')\n",
        "\n",
        "# Read in the shapefile for U.S. state boundaries\n",
        "shapefile_path = 'C:/Users/Shreya Work/OneDrive/Documents/GitHub/PS5/cb_2018_us_state_500k.shp'\n",
        "states_map = gpd.read_file(shapefile_path).to_crs(epsg=4326)\n",
        "\n",
        "# Merge the state-level enforcement action data with the states shapefile\n",
        "states_map = states_map.merge(state_action_counts, left_on='NAME', right_on='State', how='left').fillna(0)\n",
        "\n",
        "# Exclude non-continental U.S. states and territories from the map\n",
        "excluded_states = ['AS', 'HI', 'PR', 'VI', 'MP', 'GU', 'AK']\n",
        "states_map = states_map[~states_map['STUSPS'].isin(excluded_states)]\n",
        "\n",
        "# Plot the choropleth map for the enforcement actions by state\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "states_map.plot(column='action_count', cmap='Blues', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)\n",
        "\n",
        "# Adjust map extent to focus on the continental U.S.\n",
        "ax.set_xlim([-130, -65])  # Longitude range for the continental U.S.\n",
        "ax.set_ylim([24, 50])  # Latitude range for the U.S.\n",
        "\n",
        "# Add state abbreviations or full state names to the map for clarity\n",
        "for idx, row in states_map.iterrows():\n",
        "    # Use the centroid of each state for label placement\n",
        "    plt.text(row.geometry.centroid.x, row.geometry.centroid.y, \n",
        "             row['STUSPS'],  # Change to 'State' for full state names if preferred\n",
        "             fontsize=8, ha='center', color='black')\n",
        "\n",
        "# Add a title to the map and display it\n",
        "ax.set_title('Enforcement Actions by State')\n",
        "plt.show()"
      ],
      "id": "d6332590",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Map by District (PARTNER 2)\n"
      ],
      "id": "407e6b05"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import necessary libraries\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Filter the enforcement actions related to US Attorney District-level agencies\n",
        "district_data = enforcement_df_Jan_2021[enforcement_df_Jan_2021['Agency'].str.contains(\"District\", na=False)]\n",
        "\n",
        "# Extract and clean district names from the 'Agency' column\n",
        "district_data['District'] = district_data['Agency'].str.split(',').str[1].str.strip()\n",
        "\n",
        "# Aggregate the enforcement actions by district\n",
        "district_action_counts = district_data.groupby('District').size().reset_index(name='action_count')\n",
        "\n",
        "# Load the US Attorney District shapefile\n",
        "district_shapefile_path = 'C:/Users/Shreya Work/OneDrive/Documents/GitHub/PS5/geo_export_6122d5c7-042c-4e6e-9edd-c15058235201.shp'\n",
        "district_map = gpd.read_file(district_shapefile_path)\n",
        "\n",
        "# Merge the enforcement data with the district shapefile based on judicial district names\n",
        "district_shapefile.columns = district_shapefile.columns.str.strip()  # Strip any extra spaces from column names\n",
        "district_map_merged = pd.merge(district_shapefile, district_action_counts, left_on='judicial_d', right_on='District', how='left').fillna(0)\n",
        "\n",
        "# Exclude non-continental U.S. regions and territories for a more focused map\n",
        "excluded_districts = ['AS', 'HI', 'PR', 'VI', 'MP', 'GU', 'AK']\n",
        "district_map_merged = district_map_merged[~district_map_merged['abbr'].isin(excluded_districts)]\n",
        "\n",
        "# Plot the choropleth for enforcement actions by district\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "district_map_merged.plot(column='action_count', cmap='Blues', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)\n",
        "\n",
        "# Set limits for a zoomed-in view of the continental U.S.\n",
        "ax.set_xlim([-130, -65])  # Longitude range for the continental U.S.\n",
        "ax.set_ylim([24, 50])  # Latitude range for the U.S.\n",
        "\n",
        "# Annotate the map with district codes or names\n",
        "for idx, row in district_map_merged.iterrows():\n",
        "    plt.text(row.geometry.centroid.x, row.geometry.centroid.y, \n",
        "             row['abbr'],  # Use 'District' for full district names if needed\n",
        "             fontsize=8, ha='center', color='black')\n",
        "\n",
        "# Add title and show the plot\n",
        "ax.set_title('Enforcement Actions by US Attorney Districts')\n",
        "plt.show()"
      ],
      "id": "f56471c5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extra Credit\n",
        "\n",
        "### 1. Merge zip code shapefile with population"
      ],
      "id": "8ea0be75"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Conduct spatial join"
      ],
      "id": "8d1e11cd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Map the action ratio in each district"
      ],
      "id": "4ac7171b"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\Shreya Work\\AppData\\Roaming\\Python\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}